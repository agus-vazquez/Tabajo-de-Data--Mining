{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2149932",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ee93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc531685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo CSV llamado \"playlist_lyrics_limpio.csv\" y guardamos en el DataFrame df\n",
    "df = pd.read_csv(\"playlist_lyrics_limpio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dda526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función para limpiar texto eliminando HTML, signos de puntuación y espacios extra\n",
    "def limpieza_basica(df):\n",
    "    if pd.isnull(df): # Si el valor es nulo, devuelve una cadena vacía\n",
    "        return \"\"\n",
    "    texto = df.lower()                                # Convierte todo el texto a minúsculas\n",
    "    texto = re.sub(r'<.*?>', ' ', texto)              # Eliminamos etiquetas HTML\n",
    "    texto = re.sub(r\"[^a-z\\s]\", \" \", texto)           # Eliminamos todo lo que no sea letras o espacios\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()        # Reemplazamos múltiples espacios por uno solo y elimina espacios iniciales/finales\n",
    "    return texto\n",
    "\n",
    "# Aplica la limpieza de texto a la columna de letras de canciones para prepararla para el análisis\n",
    "df['letra_limpia'] = df['lyrics'].apply(limpieza_basica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna con los tokens\n",
    "df['tokens'] = df['letra_limpia'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42496f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que elimina las stopwords de una lista de palabras\n",
    "def quitar_stopwords(palabras):\n",
    "    return [p for p in palabras if p not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Aplicamos la función para quitar stopwords a la columna de tokens y guarda el resultado en una nueva columna\n",
    "df['letra_sin_stopwords'] = df['tokens'].apply(quitar_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el lematizador de NLTK (WordNet)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Función de lematización que recibe una lista y devuelve una lista, aplica lematización a cada palabra\n",
    "def solo_lemmatize(palabras):\n",
    "    if isinstance(palabras, list):  # Verifica que sea una lista\n",
    "        return [lemmatizer.lemmatize(p) for p in palabras]  # Lematiza cada palabra\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Aplicamos la lematización a las letras sin stopwords y guarda el resultado en una nueva columna\n",
    "df['letra_lema'] = df['letra_sin_stopwords'].apply(solo_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "letra_final = df['letra_lema'].apply(lambda x: \" \".join(x))  # Convertimos la lista de palabras lematizadas en un solo string por canción\n",
    "vectorizer = TfidfVectorizer()      # Inicializa el vectorizador TF-IDF\n",
    "X_tfidf = vectorizer.fit_transform(letra_final)     # Ajusta y transforma el texto en una matriz de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de73bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos K-Means para agrupar las canciones en 4 clusters basados en sus letras vectorizadas\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# Muostramos cuántas canciones hay en cada cluster\n",
    "print(df['cluster'].value_counts())\n",
    "\n",
    "# Mostramos ejemplos de canciones por cada cluster\n",
    "for i in range(4):\n",
    "    print(f\"\\n Cluster {i} - Ejemplos:\")\n",
    "    print(df[df['cluster'] == i]['title'].head(7))  # Muestra los primeros 7 títulos del cluster i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9203bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula las coordenadas de los centroides de los 4 clusters\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Calcula una matriz de distancias euclidianas entre cada par de centroides\n",
    "dist_matrix = np.linalg.norm(centroids[:, np.newaxis] - centroids, axis=2)\n",
    "\n",
    "# Genera un heatmap que muestra gráficamente las distancias entre clusters\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(dist_matrix, annot=True, cmap=\"YlGnBu\", xticklabels=[f'C{i}' for i in range(4)], yticklabels=[f'C{i}' for i in range(4)])\n",
    "plt.title(\"Distancia entre Clusters (centroides)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e63e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos los valores individuales de Silhouette para cada punto (qué tan bien está asignado al cluster)\n",
    "silhouette_vals = silhouette_samples(X_tfidf, df['cluster'])\n",
    "df['silhouette'] = silhouette_vals    # Guarda los valores en el DataFrame\n",
    "\n",
    "# Número total de clusters detectados\n",
    "n_clusters = df['cluster'].nunique()\n",
    "\n",
    "# Creaamos el gráfico de Silhouette por cluster\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "y_lower = 10  # Posición inicial vertical (eje y)\n",
    "for i in range(n_clusters):\n",
    "    # Extraemos y ordenamos los valores de silhouette del cluster actual\n",
    "    cluster_vals = silhouette_vals[df['cluster'] == i]\n",
    "    cluster_vals.sort()\n",
    "\n",
    "    size_cluster_i = len(cluster_vals)\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    # Asignamos un color único al cluster y dibuja su área\n",
    "    color = plt.cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_vals,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Agregamos la etiqueta del cluster\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, f'Cluster {i}')\n",
    "\n",
    "    y_lower = y_upper + 10  # espacio entre clusters\n",
    "\n",
    "# Línea roja vertical indicando el promedio global de Silhouette\n",
    "sil_avg = silhouette_score(X_tfidf, df['cluster'])\n",
    "ax.axvline(sil_avg, color=\"red\", linestyle=\"--\", label=f'Media global: {sil_avg:.2f}')\n",
    "\n",
    "# Ajustes del gráfico\n",
    "ax.set_title(\"Silhouette Plot por Cluster\", fontsize=14)\n",
    "ax.set_xlabel(\"Silhouette Coefficient\")\n",
    "ax.set_ylabel(\"Puntos de muestra\")\n",
    "ax.set_xlim([-0.1, 1])\n",
    "ax.set_yticks([])\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos el listado de todas las palabras usadas en el vectorizador TF-IDF\n",
    "palabras = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Obtenemos los centroides del modelo KMeans\n",
    "centroides = kmeans.cluster_centers_\n",
    "\n",
    "# Mostramos las top 10 palabras más importantes por cluster\n",
    "top_n = 10\n",
    "\n",
    "for i, centroide in enumerate(centroides):\n",
    "    print(f\"\\n Cluster {i} - Palabras clave:\")\n",
    "    # Ordenamos los índices de las palabras según su peso en el centroide, de mayor a menor\n",
    "    top_indices = centroide.argsort()[::-1][:top_n]\n",
    "    # Obtenemos las palabras correspondientes a esos índices\n",
    "    top_palabras = [palabras[ind] for ind in top_indices]\n",
    "    # Imprime las palabras clave separadas por coma\n",
    "    print(\", \".join(top_palabras))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
